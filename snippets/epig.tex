\section{EPIG}
\newcommand{\EPIG}{{\text{EPIG}}}
\subsection{EPIG}
We provide an expression for EPIG \cite{pmlr-v206-bickfordsmith23a} in the case
of GP. We begin with the problem setup. We have
\begin{itemize}
    \item $x$ represents the upwind solar wind conditions which are
    drawn from some test distribution (this may or may not be associated with
    our Bayesian priors). We assume $x$ is drawn from some test distribution
    $x \sim p(x)$
    \item $y$ represents the results if we were to run the simulator at $x$
    \item $x_T$ are our training points
    \item $y_T$ are the results of running the simulator at $x_T$
\end{itemize}
Furthermore, we assume a GP surrogate, that is \[ 
    y_T, y | x, x_T \sim N(0, K(x,x_T))
\]
For now, we assume that we are simply estimating kernel hyperparamters with
type II MLE, so that we are not in the fully Bayesian setting, but this can be easily
extended.

The EPIG acquisition function is the given by the expected information gain over
some test distribution for x
\begin{align}
    \EPIG(x_T) &= \E I(x, y; y_T | x_T) = \E[H[y|x]-H[y|x,x_T,y_T]] \\
    &= \E[I(y;y_T|x,x_T)]
\end{align}
Where the expectation is taken over $x$. We break down this information gain as
follows 
\begin{align}
    I(y;y_T|x,x_T) = H(y|x,x_T)+H(y_T|x,x_T)-H(y,y_T|x,x_T)
\end{align}
In this expression, expectations are not taken over $x$. We analyze this term by
term. The first term is independent of $x_T$ and so we can treat it as a
constant. The second term is the entropy of a multivariate normal which gives us
$\log |K(x_T)+\sigma^2 I|$. The final term is also the entropy of a multivariate normal
giving $-\log|K(x,x_T)+\sigma^2 I|$. Taking expectations over $x$ we find that
up to affine transformation
\[ 
    \EPIG(x) = \log|K(x_T) + \sigma^2 I| - \E\log|K(x,x_T) + \sigma^2 I|
\]
We see that we get two terms. The first term is our classical information gain
term which drives diversity in training points. The second measures alignment
with future inputs, and is maximized when our test point is close to training
points.

% \begin{align}
%     I(y; y_T) &= H(y) + H(y_T) - H(y, y_T) \\
% \end{align}
% We analyze each term individually. Since $H(y)$ does not depend on the training
% points we can ignore it. Given the training points, $y_T \sim \Norm(0, K_T)$ and
% so $H(y_T) = \log|K_T|$. Note that if we stopped now, we would just have the
% traditional information gain. However, we must now consider how much that
% information tells us about future observations. This requires the computation
% of $H(y, y_T)$. However, this is not as straightforward as for $H(y)$ because
% $(y, y_T)$ is only multivariate normal once conditioned on $x$. However,
% $x$ and $y_T$ are independent the end result is the same
% \begin{align}
%     H(y, y_T) &= H(y, y_T | x) + I(x;y, y_T) \\
%     &= H(y, y_T | x) + I(x;y) \\
%     &= \E\log|K'_T(x)| + C
% \end{align}
% Where we use $K'_T(x)$ to denote the augmented covariance matrix with the point
% $x$ added. Our optimization problem then becomes the problem of optimizing over
% $x_T$ the objective function
% \[ 
%     J(x_T) := \log|K_T| - \E\log|K_T'(x)|
% \]
\subsection{EPIG in the fully Bayesian context}
In the Bayesian context, we have additional uncertainty associated with the
hyperparamters $\theta$. In this case we modify the above derivation as
follows. For brevity we omit the $x$ and $x_T$ in the notation
\begin{itemize}
    \item $H(y,y_T) = H(y,y_T|\theta)+H(\theta) = \E_\theta \log|K_\theta(x,x_T)+\sigma^2 I| + H(\theta)$
    \item $H(y_T) = H(y_T|\theta)+H(\theta) = \E_\theta \log|K_\theta(x_T)+\sigma^2 I| + H(\theta)$ 
\end{itemize}
And thus 
\[
    \EPIG(x_T) &= \E_\theta \log|K_\theta(x_T)+\sigma^2 I| 
    - \E_{x,\theta} \log|K_\theta(x,x_T)+\sigma^2 I|
\]

\subsection{Extending EPIG}
EPIG might be extended by consideration of reduction uncertainty in inverse
problems. 
\subsection{TODOs}
\begin{itemize}
    \item Check that this formulation still satisfies submodularity
    \item Problem with EPIG: discrete spaces - could get $-\infty$
\end{itemize}