\section{EPIG}
\newcommand{\EPIG}{{\text{EPIG}}}
We provide an expression for EPIG \cite{pmlr-v206-bickfordsmith23a} in the case
of GP. We begin with the problem setup. We have
\begin{itemize}
    \item $x$ represents the upwind solar wind conditions which are
    drawn from some test distribution (this may or may not be associated with
    our Bayesian priors). We assume $x$ is drawn from some test distribution
    $x \sim p(x)$
    \item $y$ represents the results if we were to run the simulator at $x$
    \item $y_T$ are our training points
\end{itemize}
Furthermore, we assume a GP surrogate, that is \[ 
    y_T, y | x, x_T \sim N(0, K_x)
\]
For now, we assume that we are simply estimating kernel hyperparamters with
MLEII, so that we are not in the fully Bayesian setting, but this can be easily
extended.

The EPIG acquisition function is the given by the mutual information between
the grid values at the test points and the training points
\[ 
    \EPIG(x_T) = I(y; y_T)
\]
We expand this expression based on our model with the goal of optimizing over
the set of training points.
\begin{align}
    I(y; y_T) &= H(y) + H(y_T) - H(y, y_T) \\
\end{align}
We analyze each term individually. Since $H(y)$ does not depend on the training
points we can ignore it. Given the training points, $y_T \sim \Norm(0, K_T)$ and
so $H(y_T) = \log|K_T|$. Note that if we stopped now, we would just have the
traditional information gain. However, we must now consider how much that
information tells us about future observations. This requires the computation
of $H(y, y_T)$. However, this is not as straightforward as for $H(y)$ because
$(y, y_T)$ is only multivariate normal once conditioned on $x$. However,
$x$ and $y_T$ are independent the end result is the same
\begin{align}
    H(y, y_T) &= H(y, y_T | x) + I(x;y, y_T) \\
    &= H(y, y_T | x) + I(x;y) \\
    &= \E\log|K'_T(x)| + C
\end{align}
Where we use $K'_T(x)$ to denote the augmented covariance matrix with the point
$x$ added. Our optimization problem then becomes the problem of optimizing over
$x_T$ the objective function
\[ 
    J(x_T) := \log|K_T| - \E\log|K_T'(x)|
\]


\subsection{Extending EPIG}
EPIG might be extended by consideration of reduction uncertainty in inverse
problems. 
\subsection{TODOs}
\begin{itemize}
    \item Check that this formulation still satisfies submodularity
\end{itemize}